---
title: 'DQN (Deep Q-Network) 读书笔记'
date: 2025-11-13
permalink: /posts/2025/11/dqn/
tags:
  - reinforcement learning
  - deep learning
  - DQN
---

<p><b>作者</b>: Mnih, Kavukcuoglu, Silver, et al.<br>
<b>论文</b>: arXiv:1312.5602 / Nature 2015<br>
<b>年份</b>: 2013 (预印本)
</p>

---

<p><b>核心动机</b></p>
<ul>
  <li>Q-learning 理论上可以求解最优策略，但直接用在高维状态（如像素）上会失败，因为：</li>
  <ul>
    <li>特征需要人工设计。</li>
    <li>使用表格直接存储 Q 值，这在高维状态空间中一般都不可行。</li>
  </ul>
  <li>DQN 的目标：<b>端到端</b>，从 <b>原始像素输入</b>直接学到控制策略。</li>
</ul>

---

<p><b>基本框架</b></p>
<p>Q-learning 回顾：</p>
Q(s,a) \approx r + \gamma \max_{a'} Q(s',a')
$$
Q(s,a) \approx r + \gamma \max_{a'} Q(s',a')
$$
<p>策略：</p>
\pi(s) = \arg\max_a Q(s,a)
$$
\pi(s) = \arg\max_a Q(s,a)
$$

<p>DQN 用一个卷积神经网络表示 $Q(s,a)$：</p>
<ul>
  <li>输入：最近 4 帧预处理图像堆叠 \((84\times84\times4)\)</li>
  <li>输出：每个动作的 Q 值</li>
</ul>
<p>网络结构（Atari 版本）：</p>
<pre>
Conv(16 filters, 8x8, stride 4) + ReLU
Conv(32 filters, 4x4, stride 2) + ReLU
FC(256) + ReLU
FC(#actions)
</pre>

---

<p><b>稳定训练的关键技巧</b></p>
<ol>
  <li><b>经验回放（Experience Replay）</b><br>
    维护一个 replay buffer 存储 transition：\((s, a, r, s')\)<br>
    训练时从 buffer 中 <b>随机采样</b>，好处：
    <ul>
      <li>打破时间相关性</li>
      <li>平滑训练分布</li>
      <li>提高样本利用效率</li>
    </ul>
  </li>
  <li><b>目标网络（Target Network）</b><br>
    维护两个网络：<br>
    在线网络 \(Q_{\theta}\)<br>
    目标网络 \(Q_{\theta^-}\)<br>
    每隔 \(K\) 步复制参数：
    $$
    	heta^- \leftarrow \theta
    $$
    训练目标：
    $$
    y = r + \gamma \max_{a'} Q_{\theta^-}(s',a')
    $$
    $$
    \min_\theta (y - Q_\theta(s,a))^2
    $$
    目的：避免 Q-learning 中目标同时变化导致训练振荡。
  </li>
</ol>

---

<p><b>训练与探索</b></p>
<ul>
  <li><b>ε-greedy 策略</b><br>
    $$
    \pi(a|s) =
    \begin{cases}
    	ext{random} & \text{with probability } \epsilon \\
    \arg\max_a Q(s,a) & \text{with probability } 1-\epsilon
    \end{cases}
    $$
  </li>
  <li><b>奖励折扣</b><br>
    折扣因子 \(\gamma \in (0,1)\) 控制长期回报权重。
  </li>
</ul>

---

<p><b>Atari 实验要点</b></p>
<ul>
  <li>所有游戏使用 <b>同一套网络结构和超参数</b>，无需每个游戏单独调参。</li>
  <li>输入统一处理：
    <ul>
      <li>转灰度</li>
      <li>Resize to 84×84</li>
      <li>堆叠 4 帧</li>
    </ul>
  </li>
  <li>在多数游戏上性能超过之前所有方法，在一些游戏上达到或超过人类水平。</li>
</ul>

---

<p><b>算法总结（伪代码）</b></p>
<pre>
Initialize replay buffer D
Initialize Q network with parameters \(\theta\)
Initialize target network \(Q_{\theta^-}\) with parameters \(\theta^- = \theta\)

Repeat for each environment step:
    Observe (s, a, r, s')
    Store (s, a, r, s') into D

    Sample minibatch from D
    Compute y = r + γ max_a' Q^-(s',a')
    Update θ by minimizing (y - Q(s,a))^2

    Every K steps: θ^- ← θ
</pre>

---

<p><b>训练实验</b></p>
<p>我们使用 Atari Cartpole 环境对 DQN 算法进行了测试。以下是训练结果：</p>
<img src="/images/cart.png" alt="DQN Training Results" />
<p>其中奖励最大值是500.0（超过这个游戏会被截停），一般训练的随机奖励大概不到300。</p>
<p>训练过程的图像：</p>
<img src="/images/path.png" alt="DQN Training Process" />
<p>可以看到，很不到几百个回合就能到达一个较好的 Total Reward。</p>

---

<p><b>相关工作</b></p>
<p>这是一个 off-policy 的值函数方法，使用了深度学习的 CNN 来处理高维输入，并结合了经验回放和目标网络来稳定训练。DQN 的提出标志着深度强化学习的一个重要里程碑，开启了后续大量研究工作，如 Double DQN、Dueling DQN、Prioritized Experience Replay 等。</p>

---

<p><b>一句话总结</b></p>
<p><b>DQN = CNN 提取像素特征 + Q-learning 更新 + Replay 去相关 + Target Network 稳定训练。</b></p>
<p>即：稳定地将深度学习与强化学习结合，实现了从像素端到端学习策略控制的突破。</p>
